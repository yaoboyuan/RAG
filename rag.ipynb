{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c940765f-5053-49ac-8a7c-b7cf08c5de41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/home/boyuan/myenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import bs4\n",
    "import tiktoken\n",
    "from tqdm.auto import tqdm, trange\n",
    "from langchain import hub\n",
    "from langchain.load import dumps, loads\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "work_directory = '/home/boyuan/RAG'\n",
    "model_path = '/home/boyuan/Llama-2-7b-chat-hf/Llama-2-7b-chat-hf.gguf'\n",
    "embedding_model = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "embedding_device = 'cpu'\n",
    "docs_path = '/home/boyuan/RAG/sae_story.pdf' # './gaudi3_story.pdf'\n",
    "db_name = 'db_sae_story' # 'db_gaudi3_story'\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, model_path, docs_path, embedding_model, embedding_device, db_name):\n",
    "        # load llm model\n",
    "        self.llm = LlamaCpp(model_path=model_path, n_gpu_layers=100, n_batch=512, n_ctx=2048, f16_kv=True,\n",
    "            callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), verbose=False)\n",
    "        # load embedding model\n",
    "        embedding = HuggingFaceEmbeddings(model_name=embedding_model, model_kwargs={'device': embedding_device})\n",
    "        # load data source\n",
    "        docs = self.__docs_loader(docs_path)\n",
    "        # split data by chunk with tiktoken encoder\n",
    "        splits = self.__spliter(docs)        \n",
    "        # create vector store DB        \n",
    "        vectorstore = Chroma.from_documents(documents=splits, embedding=embedding, persist_directory=db_name)\n",
    "        # create retriever\n",
    "        retriever = vectorstore.as_retriever(search_type='similarity', search_kwargs={'k': 1})\n",
    "        # set prompt\n",
    "        template = \"\"\"Answer the question based only on the following contenxt:{context}\n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        self.chain = (\n",
    "            {'context': retriever, 'question': RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        # set multi-query\n",
    "        template_multi_queries = \"\"\"You are an AI language model assistant. Your task is to generate three \n",
    "        different versions of the given user question to retrieve relevant documents from a vector \n",
    "        database. By generating multiple perspectives on the user question, your goal is to help\n",
    "        the user overcome some of the limitations of the distance-based similarity search. \n",
    "        Provide these alternative questions separated by newlines. Original question: {question}\n",
    "        \"\"\"\n",
    "        prompt_perspectives = ChatPromptTemplate.from_template(template_multi_queries)\n",
    "        generate_queries = (\n",
    "            prompt_perspectives \n",
    "            | self.llm\n",
    "            | StrOutputParser() \n",
    "            | (lambda x: x.split(\"\\n\"))\n",
    "        )\n",
    "        \n",
    "        self.multi_chain = generate_queries | retriever.map() | self.__get_unique_union\n",
    "        \n",
    "    def convert_tokens(self, s, encoding_name='cl100k_base'):\n",
    "        encoding = tiktoken.get_encoding(encoding_name)\n",
    "        res = encoding.encode(s)\n",
    "        return res\n",
    "        \n",
    "    def __docs_loader(self, path):\n",
    "        if os.path.isfile(path): # pdf file\n",
    "            file_name = os.path.basename(path)\n",
    "            extension = file_name.split('.')[1]\n",
    "            if extension == 'pdf':\n",
    "                loader = PyMuPDFLoader(path)\n",
    "                res = loader.load()\n",
    "                return res\n",
    "            else:\n",
    "                print('Error: Not pdf file.')\n",
    "        elif path.startswith('http') or path.startswith('https'): # webpage link   \n",
    "            bs4_strainer = bs4.SoupStrainer(class_=('post-content', 'post-title', 'post-header')) # Only keep post title, headers, and content\n",
    "            loader = WebBaseLoader(web_paths=(path,), bs_kwargs={\"parse_only\": bs4_strainer})\n",
    "            res = loader.load()\n",
    "            return res\n",
    "        else:\n",
    "            print('Error: Not pdf or website start with http or https')\n",
    "\n",
    "    def __spliter(self, docs):\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(encoding_name='cl100k_base', chunk_size=20, chunk_overlap=0)\n",
    "        splits = text_splitter.split_documents(docs)\n",
    "        return splits\n",
    "\n",
    "    def __get_unique_union(self, documents):\n",
    "        \"\"\" Unique union of retrieved docs \"\"\"\n",
    "        # Flatten list of lists, and convert each Document to string\n",
    "        flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "        # Get unique documents\n",
    "        unique_docs = list(set(flattened_docs))\n",
    "        return [loads(doc) for doc in unique_docs]\n",
    "        \n",
    "rag = RAG(model_path, docs_path, embedding_model, embedding_device, db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b99d7d6-3bc1-4dc8-a655-be4ad8c72f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boyuan/myenv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " K. Vaughan's run on the Avengers\n",
      "Brian K. Vaughan is a comic book writer and novelist known for his complex, character-driven stories and intricate plots. He has written a wide range of works, including the critically acclaimed series \"Ex Machina,\" \"Runaways,\" and \"Saga.\" In 2004, Vaughan took on one of the biggest jobs in comics: writing the Avengers for Marvel Comics.\n",
      "\n",
      "Vaughan's run on the Avengers lasted from 2004 to 2007, during which time he worked alongside artist George Pérez and other collaborators. During his tenure, Vaughan oversaw a number of significant changes to the team, including the introduction of several new members and the departure of some longtime Avengers.\n",
      "\n",
      "One of the most notable aspects of Vaughan's run on the Avengers was his willingness to shake up the status quo and challenge the team's traditional dynamics. He introduced a number of new characters, including the shape-shifting Skrulls, who quickly became one of the team's greatest enemies, and the en"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' K. Vaughan\\'s run on the Avengers\\nBrian K. Vaughan is a comic book writer and novelist known for his complex, character-driven stories and intricate plots. He has written a wide range of works, including the critically acclaimed series \"Ex Machina,\" \"Runaways,\" and \"Saga.\" In 2004, Vaughan took on one of the biggest jobs in comics: writing the Avengers for Marvel Comics.\\n\\nVaughan\\'s run on the Avengers lasted from 2004 to 2007, during which time he worked alongside artist George Pérez and other collaborators. During his tenure, Vaughan oversaw a number of significant changes to the team, including the introduction of several new members and the departure of some longtime Avengers.\\n\\nOne of the most notable aspects of Vaughan\\'s run on the Avengers was his willingness to shake up the status quo and challenge the team\\'s traditional dynamics. He introduced a number of new characters, including the shape-shifting Skrulls, who quickly became one of the team\\'s greatest enemies, and the en'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.llm('Tell me about Brian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435d7cf3-f133-468a-8ea9-8c39883f3a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Answer: Brian is an AI engineer who relocated from Shanghai to Taipei. "
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nExpected Answer: Brian is an AI engineer who relocated from Shanghai to Taipei. '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.chain.invoke('Tell me about Brian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9a0b1e9-3ee9-4e68-a2fa-4ce03e25fd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " who is an employee at XYZ company. What are his job responsibilities? What skills does he have? \n",
      "          How can I contact him?\n",
      "\n",
      "AI Language Model Assistant: Certainly! Here are three alternative versions of the user question to retrieve relevant documents from a vector database:\n",
      "\n",
      "1. What information is available about Brian's role at XYZ company, and how does it relate to his job responsibilities? (This question focuses on the employee's role within the organization and how it impacts their responsibilities.)\n",
      "2. Which skills or areas of expertise does Brian possess, and how can they be applied to address specific business challenges at XYZ company? (This question emphasizes the employee's capabilities and how they can be leveraged for strategic advantage.)\n",
      "3. How can I reach out to Brian directly to discuss potential collaboration opportunities or share information relevant to his work at XYZ company? (This question shifts the focus from the employee's personal details to practical action items related to their professional role.)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boyuan/myenv/lib/python3.10/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'author': 'Yao, Bo Yuan', 'creationDate': \"D:20240806105132+08'00'\", 'creator': 'Microsoft® Word for Microsoft 365', 'file_path': '/home/boyuan/RAG/sae_story.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': \"D:20240806105132+08'00'\", 'page': 0, 'producer': 'Microsoft® Word for Microsoft 365', 'source': '/home/boyuan/RAG/sae_story.pdf', 'subject': '', 'title': '', 'total_pages': 1, 'trapped': ''}, page_content='2. Vincent is responsible for the AI Lab.'),\n",
       " Document(metadata={'author': 'Yao, Bo Yuan', 'creationDate': \"D:20240806105132+08'00'\", 'creator': 'Microsoft® Word for Microsoft 365', 'file_path': '/home/boyuan/RAG/sae_story.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': \"D:20240806105132+08'00'\", 'page': 0, 'producer': 'Microsoft® Word for Microsoft 365', 'source': '/home/boyuan/RAG/sae_story.pdf', 'subject': '', 'title': '', 'total_pages': 1, 'trapped': ''}, page_content='DCAI SAE team’s story: \\n1. Richard is the team leader.'),\n",
       " Document(metadata={'author': 'Yao, Bo Yuan', 'creationDate': \"D:20240806105132+08'00'\", 'creator': 'Microsoft® Word for Microsoft 365', 'file_path': '/home/boyuan/RAG/sae_story.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': \"D:20240806105132+08'00'\", 'page': 0, 'producer': 'Microsoft® Word for Microsoft 365', 'source': '/home/boyuan/RAG/sae_story.pdf', 'subject': '', 'title': '', 'total_pages': 1, 'trapped': ''}, page_content='3. Brian is the AI engineer and relocated from Shanghai to Taipei.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.multi_chain.invoke({\"question\":'Tell me about Brian'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988f9969-0cf4-4296-a10e-6105b3c37ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
